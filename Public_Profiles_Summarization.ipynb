{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfrnB5+sAtZPq06DHBjOxS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devsjee/internet-profile-summary/blob/main/Public_Profiles_Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, I am going to develop an end to end application that will take a person's name and identification clues and builds a summary about the person from the profiles publicly available about him/her on the Internet."
      ],
      "metadata": {
        "id": "HeWsyclI5wlG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rough outline of how I am going to proceed:\n",
        "\n",
        "*   Figure out the input and its format\n",
        "*   how to search the internet with this input\n",
        "*   how to filter out results?\n",
        "*   there might be false positives how to recognize them? may be have a manual intervention at this point of time.\n",
        "*   summary creation -- using LLM?\n",
        "*   false negatives? how much important? how to improvise (this can be handled in the second round of coding too)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YopHLT1k6J1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Figure out the input and its format**\n",
        "\n",
        "The input will definitely contain the name of the person. In some cases like for celebrities (let's say Rajinikanth) the name alone suffices to fetch relevant articles when we do a google search on the internet. But for a normal person (let's say Devi Ganesan ,which is me :) ), the name alone is less likely to fetch only the relevant articles. That is, the search results from google search engine will not only include relevant pages about me but also include pages talking about 'Actor Sivaji Ganesan', 'Goddess Devi', etc. I may not be able to eliminate all the irrelevant articles from the search but at least I would want the relevant articles to come up in the top 5 to 10 results of the search. In technical words, I would like the relevant results to have a high MAP ([Mean Average Precision](https://en.wikipedia.org/wiki/Mean_reciprocal_rank)).\n",
        "\n",
        "To begin with, I am going to assume that the input will consist of the following:\n",
        "\n",
        "1.   name of the person\n",
        "2.   place of birth/residence (optional)\n",
        "3.   profession/industry name (optional)\n",
        "4.   educational institute (optional)\n",
        "5.   any other association that would be of significance (optional)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "II5hODyxJNrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"For creating the profile summary, please enter the following details\")\n",
        "name = input(\"Name of the person (mandatory): \")\n",
        "place = input(\"birth/residence place (optional): \")\n",
        "profession = input(\"profession/industry name (optional): \")\n",
        "edu_inst = input(\"educational institute (optional): \")\n",
        "other = input(\"any other significant association (optional): \")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QdPqwE53kYT",
        "outputId": "7b60891a-c585-4da9-ac4f-289852cf78dc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For creating the profile summary, please enter the following details\n",
            "Name of the person (mandatory): devi ganesan\n",
            "birth/residence place (optional): chennai\n",
            "profession/industry name (optional): accenture\n",
            "educational institute (optional): iitm\n",
            "any other significant association (optional): wellington\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \" \".join([name,edu_inst, place, profession, other])\n",
        "print(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZX-WtXS2-Rab",
        "outputId": "31aca669-1ba0-42d5-92c8-424ec9f1189a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "devi ganesan iitm chennai accenture wellington\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Let's do a web search with the input and gather some results\n",
        "#!pip install beautifulsoup4\n",
        "#!pip install google\n",
        "\n",
        "try:\n",
        "  from googlesearch import search\n",
        "except ImportError:\n",
        "  print(\"No module named google present. Please install 'google' package using pip\")\n",
        "else:\n",
        "  print('successfully imported')\n",
        "\n",
        "selected_webpages=[]\n",
        "\n",
        "#I am manually selecting those urls from which I want my profile summary to be created -- for experimentation purpose\n",
        "for webpage in (search(query,tld=\"com\", num=10,lang='en',stop=10, pause=2)):\n",
        "  sel= input(webpage+\" (Y/N) : \")\n",
        "  if sel == 'y' or sel == 'Y':\n",
        "    selected_webpages.append(webpage)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUll-OuT6Idk",
        "outputId": "a15c3660-cf0e-4bb3-c6ce-618cb4272e35"
      },
      "execution_count": 25,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "successfully imported\n",
            "https://in.linkedin.com/in/devi-g-310190215 (Y/N) : \n",
            "http://www.cse.iitm.ac.in/~gdevi/ (Y/N) : y\n",
            "https://in.linkedin.com/in/devi-ganesan-52226641 (Y/N) : \n",
            "https://www.researchgate.net/profile/Devi_Ganesan (Y/N) : \n",
            "https://issuu.com/hindustanuniversitypublications/docs/hcellence_magazine_2022 (Y/N) : \n",
            "https://www.inae.in/storage/2022/05/INAE_Year_Book_2022_low.pdf (Y/N) : \n",
            "https://www.iitsystem.ac.in/sites/default/files/annualreport/3/IITM-AnnualReport-2015-16.pdf (Y/N) : \n",
            "http://ioa-dst.pec.ac.in/all-academicians?page=883 (Y/N) : \n",
            "https://ipindia.gov.in/writereaddata/Portal/IPOJournal/1_5007_1/Part-2.pdf (Y/N) : \n",
            "https://www.iodglobal.com/masterclass/get_more_data (Y/N) : \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(selected_webpages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6T-zlRyq895_",
        "outputId": "05050241-a983-4355-aa95-92e72eab2349"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['http://www.cse.iitm.ac.in/~gdevi/']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, till this point, I have been able to do a google search for the input query and manually select the relevant urls from a list of search results obtained from google query. Next, I am going to use the WebBasedLoader from langchain framework to fetch the page-contents of the above urls and then use a LLM model (here it is Gemini-pro) again via the langchain framework to create a summary of the fetched pages.\n",
        "\n",
        "\n",
        "Langchain for text summarization has been already explored [in this colab book](https://colab.research.google.com/gist/devsjee/8bee27534e5a263deb35f05d4ee6b28f/text-summarization-using-langchain-and-gemini-llm.ipynb?authuser=2#scrollTo=zesF2hABSdnV) and the following is a simple adaption of the same."
      ],
      "metadata": {
        "id": "nJGIGWPLxVqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#uncomment the following lines and run this cell if langchain and langchain_google_genai are not already installed\n",
        "\n",
        "#!pip install --quiet langchain_google_genai\n",
        "#!pip install --quiet langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4ZibWAOt5Q6",
        "outputId": "1e951770-e678-4718-c553-fe305588dac3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_google_genai\n",
            "  Downloading langchain_google_genai-0.0.6-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: google-generativeai<0.4.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from langchain_google_genai) (0.3.2)\n",
            "Collecting langchain-core<0.2,>=0.1 (from langchain_google_genai)\n",
            "  Downloading langchain_core-0.1.16-py3-none-any.whl (230 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/230.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.3/230.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-ai-generativelanguage==0.4.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.4.0,>=0.3.1->langchain_google_genai) (0.4.0)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.4.0,>=0.3.1->langchain_google_genai) (2.17.3)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.4.0,>=0.3.1->langchain_google_genai) (2.11.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.4.0,>=0.3.1->langchain_google_genai) (4.5.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.4.0,>=0.3.1->langchain_google_genai) (3.20.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.4.0,>=0.3.1->langchain_google_genai) (4.66.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.4.0->google-generativeai<0.4.0,>=0.3.1->langchain_google_genai) (1.23.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain_google_genai) (6.0.1)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain_google_genai) (3.7.1)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.2,>=0.1->langchain_google_genai)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1,>=0.0.83 (from langchain-core<0.2,>=0.1->langchain_google_genai)\n",
            "  Downloading langsmith-0.0.83-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain_google_genai) (23.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain_google_genai) (1.10.14)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain_google_genai) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain_google_genai) (8.2.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain_google_genai) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain_google_genai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain_google_genai) (1.2.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.2,>=0.1->langchain_google_genai)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-core<0.2,>=0.1->langchain_google_genai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-core<0.2,>=0.1->langchain_google_genai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-core<0.2,>=0.1->langchain_google_genai) (2023.11.17)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.4.0,>=0.3.1->langchain_google_genai) (1.62.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth->google-generativeai<0.4.0,>=0.3.1->langchain_google_genai) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth->google-generativeai<0.4.0,>=0.3.1->langchain_google_genai) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth->google-generativeai<0.4.0,>=0.3.1->langchain_google_genai) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth->google-generativeai<0.4.0,>=0.3.1->langchain_google_genai) (4.9)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.4.0,>=0.3.1->langchain_google_genai) (1.60.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.4.0,>=0.3.1->langchain_google_genai) (1.48.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth->google-generativeai<0.4.0,>=0.3.1->langchain_google_genai) (0.5.1)\n",
            "Installing collected packages: jsonpointer, langsmith, jsonpatch, langchain-core, langchain_google_genai\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-2.4 langchain-core-0.1.16 langchain_google_genai-0.0.6 langsmith-0.0.83\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.1.4-py3-none-any.whl (803 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.6/803.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.24)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Collecting langchain-community<0.1,>=0.0.14 (from langchain)\n",
            "  Downloading langchain_community-0.0.16-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langchain-core<0.2,>=0.1.16 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.16)\n",
            "Requirement already satisfied: langsmith<0.1,>=0.0.83 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.83)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.14)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain) (1.2.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-community, langchain\n",
            "Successfully installed dataclasses-json-0.6.3 langchain-0.1.4 langchain-community-0.0.16 marshmallow-3.20.2 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "from langchain import PromptTemplate\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.schema.prompt_template import format_document\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "#this google api key can be obtained free of cost from makersuite.com which is Google AI studio\n",
        "os.environ['GOOGLE_API_KEY'] = getpass.getpass(\"Enter the GOOGLE API KEY: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEGxqZmht3jb",
        "outputId": "f84e608f-4460-4792-d203-e7b2a95f559c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the GOOGLE API KEY: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loader = WebBaseLoader(selected_webpages)\n",
        "docs = loader.load()\n",
        "print(docs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dde6SHAfvqFL",
        "outputId": "457cf891-6874-479d-a84e-49bc5870abaf"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='\\n\\n\\n\\nDevi Ganesan\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDevi Ganesan\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nABOUT ME\\nRESEARCH\\nPUBLICATIONS\\nTALKS\\nUSEFUL LINKS\\n\\n\\n\\n\\n\\nContact\\n\\nDevi Ganesan\\r\\n\\t\\t\\t\\t\\tBSB 365,\\r\\n\\t\\t\\t\\t\\tArtificial Intelligence and Databases Lab (AIDB),\\r\\n\\t\\t\\t\\t\\tDepartment of Computer Science & Engineering,\\r\\n\\t\\t\\t\\t\\tIndian Institue of Technology Madras,\\r\\n\\t\\t\\t\\t\\tChennai-36, Tamil Nadu, India.\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\nEmail: gdevi@cse.iitm.ac.in\\n\\n\\n\\n\\n\\n\\nAbout me\\n\\n One of my favorite quotes is \"There are only two ways to live your life - one is as though nothing is a miracle, the other is as though everything is a miracle\"- by Albert Einstein. Many a time, I have lost myself in thoughts about the beauty and brilliance of Mother Nature. Hence, I sincerly believe that I belong to the latter category ;) Nevertheless, my motto - \"Never Stop Learning\" will speak for me.\\nPresent\\nI am currently engaged in doctoral research at IIT Madras. I work under the supervision of Prof.Dr.Sutanu Chakraborti in the Department of Computer Science and Engineering. In the vast sea of Artificial Intelligence, my research focus is on the confluence of classical and statistical AI. More details about my research is available  here. \\nPast\\n\\r\\n\\t\\tMasters of Technology in CSE, IIT Madras (2014-2016) \\r\\n\\t\\tPlanning & Systems Engineer, Bharat Heavy Electricals Limited -Trichy (2010-2014) \\r\\n\\t\\tBachelors of Engineering in CSE, Madras Institute of Technology, Anna University (2006-2010)\\r\\n\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Copyrights Reserved, 2017\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'http://www.cse.iitm.ac.in/~gdevi/', 'title': 'Devi Ganesan', 'description': 'Devi Ganesan', 'language': 'No language found.'})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm=ChatGoogleGenerativeAI(model=\"gemini-pro\",temperature =0.7, top_p=0.85)\n",
        "doc_prompt = PromptTemplate.from_template(\"{page_content}\")\n",
        "llm_prompt = PromptTemplate.from_template(\"Create a profile summary of the following : {text} \\n\\n Summary:\")\n",
        "\n",
        "print(doc_prompt)\n",
        "print(llm_prompt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HepoA4HtdID5",
        "outputId": "deee9acc-ce77-4e5a-d117-73e7d5854b3a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['page_content'] template='{page_content}'\n",
            "input_variables=['text'] template='Create a profile summary of the following : {text} \\n\\n Summary:'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "stuff_chain = (\n",
        "        {\"text\": lambda docs: \"\\n\\n\".join(format_document(doc, doc_prompt) for doc in docs)}\n",
        "        | llm_prompt | llm | StrOutputParser()\n",
        "        )\n",
        "\n",
        "stuff_chain.invoke(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "Q__w5QmJwdtY",
        "outputId": "9130120c-19c2-43c1-d1d8-8f1168774e85"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Devi Ganesan is a PhD researcher at the Indian Institute of Technology Madras, specializing in the intersection of classical and statistical artificial intelligence. Her research interests lie in the confluence of classical and statistical AI. She holds a Master of Technology in CSE from IIT Madras and a Bachelor of Engineering in CSE from Madras Institute of Technology, Anna University. Prior to her doctoral studies, she worked as a Planning & Systems Engineer at Bharat Heavy Electricals Limited -Trichy.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some output analysis goes here:\n",
        "\n",
        "The following are the urls I had selected for summary\n",
        "\n",
        "['https://in.linkedin.com/in/devi-g-310190215', 'http://www.cse.iitm.ac.in/~gdevi/', 'https://www.researchgate.net/profile/Devi_Ganesan']\n",
        "\n",
        "And the summary created for me is:\n",
        "\n",
        "Devi Ganesan is a PhD researcher at the Indian Institute of Technology Madras, specializing in the intersection of classical and statistical artificial intelligence. Her research interests lie in the confluence of classical and statistical AI. She holds a Master of Technology in CSE from IIT Madras and a Bachelor of Engineering in CSE from Madras Institute of Technology, Anna University. Prior to her doctoral studies, she worked as a Planning & Systems Engineer at Bharat Heavy Electricals Limited -Trichy.\n",
        "\n",
        "\n",
        "I think this is a decent summmary of my profile.\n",
        "\n",
        "---\n",
        "\n",
        "There are few things that will have to be investigated to improve the performance of this system.\n",
        "1. Construction of query from the input variables:\n",
        "A query such as 'devi ganesan' is very general whereas a query like 'devi ganesan iitm accenture wellington' is too spectific. So, both of these are likely to fetch many irrelevant results. One idea is to construct several inputs like 'devi ganesan iitm', 'devi ganesan accenture', 'devi ganesan wellington' to perform google search and then select the most useful results.\n",
        "\n",
        "2. From manual to automated filtering: Currently, I am filtering out the useful relevant links from the listed ones. One idea for automation is to select the common urls among the multiple searches explained in the previous point.\n",
        "\n",
        "3. Suppose the input is very sparse, can the LLM indulge in hallucinations? This is to be verified and if found true, one solution would be to use a vector database for narrowing down the summarization scope to the stored articles of the vector database only."
      ],
      "metadata": {
        "id": "52bfehe86QUp"
      }
    }
  ]
}